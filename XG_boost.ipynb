{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c3cb6c-1e11-460a-bcbe-5e9d0c2bf4bf",
   "metadata": {},
   "source": [
    "# Import Necessary Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cef2466-2270-4b16-83df-2088360bfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe41d94-790a-4448-8261-888a83aaf975",
   "metadata": {},
   "source": [
    "# Load Updated CSV into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae4f157-d8b9-4295-904d-55060341b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hong_et_al/df_updt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f79ece-56f5-436c-be69-edd69657a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep_name</th>\n",
       "      <th>esi</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>lang</th>\n",
       "      <th>religion</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>employstatus</th>\n",
       "      <th>...</th>\n",
       "      <th>cc_vaginaldischarge</th>\n",
       "      <th>cc_vaginalpain</th>\n",
       "      <th>cc_weakness</th>\n",
       "      <th>cc_wheezing</th>\n",
       "      <th>cc_withdrawal-alcohol</th>\n",
       "      <th>cc_woundcheck</th>\n",
       "      <th>cc_woundinfection</th>\n",
       "      <th>cc_woundre-evaluation</th>\n",
       "      <th>cc_wristinjury</th>\n",
       "      <th>cc_wristpain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>English</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Married</td>\n",
       "      <td>Not Employed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>English</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Married</td>\n",
       "      <td>Not Employed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 974 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dep_name  esi   age gender           ethnicity  \\\n",
       "0        B  4.0  40.0   Male  Hispanic or Latino   \n",
       "1        B  4.0  66.0   Male  Hispanic or Latino   \n",
       "2        B  2.0  66.0   Male  Hispanic or Latino   \n",
       "\n",
       "                                        race     lang     religion  \\\n",
       "0                         White or Caucasian  English          NaN   \n",
       "1  Native Hawaiian or Other Pacific Islander  English  Pentecostal   \n",
       "2  Native Hawaiian or Other Pacific Islander  English  Pentecostal   \n",
       "\n",
       "  maritalstatus  employstatus  ... cc_vaginaldischarge cc_vaginalpain  \\\n",
       "0        Single     Full Time  ...                 0.0            0.0   \n",
       "1       Married  Not Employed  ...                 0.0            0.0   \n",
       "2       Married  Not Employed  ...                 0.0            0.0   \n",
       "\n",
       "  cc_weakness cc_wheezing cc_withdrawal-alcohol cc_woundcheck  \\\n",
       "0         0.0         0.0                   0.0           0.0   \n",
       "1         0.0         0.0                   0.0           0.0   \n",
       "2         0.0         0.0                   0.0           0.0   \n",
       "\n",
       "  cc_woundinfection  cc_woundre-evaluation  cc_wristinjury  cc_wristpain  \n",
       "0               0.0                    0.0             0.0           0.0  \n",
       "1               0.0                    0.0             0.0           0.0  \n",
       "2               0.0                    0.0             0.0           0.0  \n",
       "\n",
       "[3 rows x 974 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial view of the frame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea957ac7-6f49-4248-abab-dba89a052022",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db66db6-3efe-47cb-90cc-3896c83178f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop CKD information columns\n",
    "df = df.drop(columns=df.filter(like='chrkidneydisease').columns)\n",
    "\n",
    "# Exclude columns describing direct parameter to CKD-EPI Creatinine Equation used to calculate eGFR\n",
    "df = df.drop(columns=df.filter(like='creat').columns)\n",
    "\n",
    "# Drop rows with NaN eGFR values\n",
    "df.dropna(subset=['egfr_CKD_EPI'])\n",
    "\n",
    "# Impute median data for any remaining NaN values in numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Cast categorical columns as type category\n",
    "categorical_cols = ['dep_name', 'gender', 'ethnicity', 'race', 'lang', 'religion',\n",
    "                    'maritalstatus', 'employstatus', 'insurance_status', 'disposition', 'arrivalmode',\n",
    "                    'arrivalmonth', 'arrivalday', 'arrivalhour_bin', 'previousdispo']\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5148bf-71c4-48ff-94e7-b1da1727fe00",
   "metadata": {},
   "source": [
    "# Define Train and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e37fb3-3b40-4001-8de9-b7b3ec8977cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude target variable from features frame\n",
    "X = df.drop(columns=df.filter(like='egfr').columns)\n",
    "\n",
    "# Set target variable to egfr_CKD_EPI; calculated patient eGFR \n",
    "y = df['egfr_CKD_EPI']\n",
    "\n",
    "def create_splits(X, y, test_size=0.1, val_size=0.2, n_splits=5, seed=42):\n",
    "    np.random.seed(seed)  # Ensure reproducibility\n",
    "    \n",
    "    # Step 1: Split data into test and the remaining data\n",
    "    X_traindev, X_test, y_traindev, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    # Step 2: Further split the remaining data into multiple train and validation sets\n",
    "    val_sets = []\n",
    "    for _ in range(n_splits):\n",
    "        # Randomly select validation set from the remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_traindev, y_traindev, test_size=val_size, random_state=np.random.randint(10000))\n",
    "        \n",
    "        val_sets.append({\n",
    "            'X_train': X_train, \n",
    "            'y_train': y_train, \n",
    "            'X_val': X_val, \n",
    "            'y_val': y_val\n",
    "        })\n",
    "\n",
    "    return X_traindev, y_traindev, X_test, y_test, val_sets\n",
    "\n",
    "X_traindev, y_traindev, X_test, y_test, val_sets = create_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0156821-4a50-42f4-beab-0a20661ec283",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1477a4c7-b87b-43e6-9752-05d4e23a83f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5e23c7-9296-480d-af58-40c5bdb65ae1",
   "metadata": {},
   "source": [
    "# Initialize XGBoost Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320f870a-dcfb-45f7-9b4a-65af5d762ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', verbosity=0, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdeb026-2bbd-47f4-980f-531e730d7501",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GridSearch Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcc28c39-445d-4185-8191-3b13f10b3742",
   "metadata": {},
   "source": [
    "def train_and_evaluate(X_train, y_train, X_val, y_val, params):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, 'eval')], early_stopping_rounds=10, verbose_eval=False)\n",
    "    preds = model.predict(dval)\n",
    "    mse = mean_squared_error(y_val, preds)\n",
    "    return mse\n",
    "\n",
    "def evaluate_parameters(val_set, params):\n",
    "    mse = train_and_evaluate(val_set['X_train'], val_set['y_train'], val_set['X_val'], val_set['y_val'], params)\n",
    "    return mse\n",
    "\n",
    "def GridSearch(hyperparameters, val_sets, n_jobs=-1):\n",
    "    best_mse = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # Prepare list of all parameter combinations\n",
    "    all_params = [dict(zip(hyperparameters.keys(), params_tuple)) for params_tuple in product(*(hyperparameters[param] for param in hyperparameters))]\n",
    "    \n",
    "    # Add fixed hyperparameters to all parameter sets\n",
    "    for params in all_params:\n",
    "        params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse'})\n",
    "\n",
    "    # Define the evaluation over all validation sets\n",
    "    def evaluate_all(val_sets, params):\n",
    "        mse_scores = Parallel(n_jobs=n_jobs)(delayed(evaluate_parameters)(val_set, params) for val_set in val_sets)\n",
    "        average_mse = sum(mse_scores) / len(mse_scores)\n",
    "        return average_mse, params\n",
    "\n",
    "    # Execute grid search in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(evaluate_all)(val_sets, params) for params in all_params)\n",
    "\n",
    "    # Identify the best parameters and MSE\n",
    "    for mse, params in results:\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_params = params\n",
    "\n",
    "    return best_mse, best_params\n",
    "\n",
    "# Example usage of the new GridSearch function\n",
    "hyperparameters = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_child_weight': [1, 2.5, 5, 7.5, 10],\n",
    "    'gamma': [0.5, 2, 3.5, 5.0],\n",
    "    'subsample': [0.05, 0.25, 0.45, 0.6],\n",
    "    'colsample_bytree': [0.05, 0.2, 0.35, 0.5],\n",
    "    'learning_rate': [0.01, 0.02, 0.03]\n",
    "}\n",
    "\n",
    "mse, params = GridSearch(hyperparameters, val_sets)\n",
    "print(f\"Best Params: {params}, Best MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0d524-a4ab-4bf8-a2f8-d8a5a88463b8",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a738058c-7e63-47db-b886-fa6ea07bdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_val_sets(model, params, val_sets):\n",
    "    mse_scores = []\n",
    "    for val in val_sets:\n",
    "        # Set parameters and reinitialize the model to avoid leakage from previous fits\n",
    "        model.set_params(**params)\n",
    "        model.fit(val['X_train'], val['y_train'])\n",
    "\n",
    "        try:\n",
    "            # Predict on the validation set and calculate MSE\n",
    "            preds = model.predict(val['X_val'])\n",
    "            mse = mean_squared_error(val['y_val'], preds)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model prediction: {str(e)}\")\n",
    "            mse = float('inf')  # Consider the worst case if prediction fails\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "    \n",
    "    # Calculate average MSE across all validation sets\n",
    "    return np.mean(mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d16a72-aac3-45e4-bd0b-bc3d4afdb4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'max_depth': 9, 'min_child_weight': 3, 'gamma': 4.008609501227463, 'subsample': 0.8387400631785948, 'colsample_bytree': 0.7783331011414365, 'learning_rate': 0.03899272558722084, 'n_estimators': 165}\n",
      "MSE for params: 79.56510509045893\n",
      "Testing params: {'max_depth': 5, 'min_child_weight': 2, 'gamma': 3.4289981282698383, 'subsample': 0.6225646316108401, 'colsample_bytree': 0.88879950890673, 'learning_rate': 0.2821802856145676, 'n_estimators': 50}\n",
      "MSE for params: 93.05953569987983\n",
      "Testing params: {'max_depth': 10, 'min_child_weight': 7, 'gamma': 3.2524392221972644, 'subsample': 0.6028265220878869, 'colsample_bytree': 0.6092249700165663, 'learning_rate': 0.16218465147493288, 'n_estimators': 150}\n",
      "MSE for params: 72.92275977378634\n",
      "Testing params: {'max_depth': 3, 'min_child_weight': 10, 'gamma': 1.5474710319363694, 'subsample': 0.6362425738131283, 'colsample_bytree': 0.847354403733235, 'learning_rate': 0.12091397746747722, 'n_estimators': 296}\n",
      "MSE for params: 94.65331144345289\n",
      "Testing params: {'max_depth': 6, 'min_child_weight': 9, 'gamma': 3.561383923645009, 'subsample': 0.7801997007878172, 'colsample_bytree': 0.6053059844639466, 'learning_rate': 0.2832385091486074, 'n_estimators': 191}\n",
      "MSE for params: 80.30768817805804\n",
      "Testing params: {'max_depth': 6, 'min_child_weight': 1, 'gamma': 1.5390222152996706, 'subsample': 0.6964101864104046, 'colsample_bytree': 0.8733054075301834, 'learning_rate': 0.1868990307569601, 'n_estimators': 258}\n",
      "MSE for params: 78.58508086825626\n",
      "Testing params: {'max_depth': 4, 'min_child_weight': 5, 'gamma': 1.3200623950462806, 'subsample': 0.9021445641270611, 'colsample_bytree': 0.7700623497964979, 'learning_rate': 0.07030308223177477, 'n_estimators': 192}\n",
      "MSE for params: 95.5759772250395\n",
      "Testing params: {'max_depth': 3, 'min_child_weight': 9, 'gamma': 2.523893600163946, 'subsample': 0.7580600944007259, 'colsample_bytree': 0.9706635463175177, 'learning_rate': 0.22090887879836207, 'n_estimators': 132}\n",
      "MSE for params: 97.63156986058269\n",
      "Testing params: {'max_depth': 7, 'min_child_weight': 6, 'gamma': 4.825274109572072, 'subsample': 0.9378135394712607, 'colsample_bytree': 0.8989280440549523, 'learning_rate': 0.16651071839283316, 'n_estimators': 197}\n",
      "MSE for params: 76.37584353713\n",
      "Testing params: {'max_depth': 10, 'min_child_weight': 6, 'gamma': 1.7419963191014454, 'subsample': 0.718509402281633, 'colsample_bytree': 0.666106775625201, 'learning_rate': 0.01453455795494624, 'n_estimators': 156}\n",
      "MSE for params: 90.9715424050485\n",
      "Best parameters found:  {'max_depth': 10, 'min_child_weight': 7, 'gamma': 3.2524392221972644, 'subsample': 0.6028265220878869, 'colsample_bytree': 0.6092249700165663, 'learning_rate': 0.16218465147493288, 'n_estimators': 150}\n",
      "Best average MSE across validation sets:  72.92275977378634\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "space = [\n",
    "    Integer(3, 20, name='max_depth'),\n",
    "    Integer(1, 10, name='min_child_weight'),\n",
    "    Real(0.5, 5.0, name='gamma'),\n",
    "    Real(0.6, 1.0, name='subsample'),\n",
    "    Real(0.6, 1.0, name='colsample_bytree'),\n",
    "    Real(0.01, 0.3, name='learning_rate'),\n",
    "    Integer(50, 300, name='n_estimators')\n",
    "]\n",
    "\n",
    "# Define objective function\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    print(\"Testing params:\", params)  # Debug: print parameters to console\n",
    "    mse = evaluate_on_val_sets(model, params, val_sets)\n",
    "    print(\"MSE for params:\", mse)  # Debug: print result to console\n",
    "    return mse\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "result = gp_minimize(objective, space, n_calls=10, random_state=42)\n",
    "\n",
    "\n",
    "# Extract the best parameters and the corresponding score\n",
    "best_params = {dimension.name: result.x[i] for i, dimension in enumerate(space)}\n",
    "best_score = result.fun\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best average MSE across validation sets: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bb7bd-5f4c-47a9-a547-9b5fc6862926",
   "metadata": {},
   "source": [
    "# Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce08fc7-652e-4f6b-8fc9-a6378cc1c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a frame for feature importance visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance frame\n",
    "print(feature_importance.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36dcac-91d2-4d31-9b82-37eadb11e8e2",
   "metadata": {},
   "source": [
    "# Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c563a-4928-4319-898f-55499a1256cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(30))\n",
    "plt.title('Feature Importances in eGFR Prediction')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
